{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/singapore/comments/cqz78l/found_in_bugis/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqpuk6/looks_familiar/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqogya/theres_a_fashion_brand_in_manchester_uk_called/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqnsay/merlion_spotted_in_xian_china/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqpsaa/i_took_a_photo_of_an_empty_bplrt/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqplqa/ah_lah_ma_you_ah/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqqexg/man_puts_selfies_of_his_pantysniffing_escapades/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqxvm0/activist_who_coined_the_term_chinese_privilege/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqoxa2/singapores_mrt_one_mans_vision/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqqnc6/bitcoin_loop_scam_is_back_on_facebook_dont_fall/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cr1gei/the_last_days_of_shaw_balestier_or_what_a_mall/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqnejd/why_nas_daily_is_wrong_about_democracy/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqn038/george_clooneys_sisterinlaw_faces_drinkdriving/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cr1rhr/man_arrested_over_lewd_instagram_posts_of_himself/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqrwd3/why_are_nus_fees_going_up_despite_billiondollar/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cr2p7j/now_we_know_why_18_years_old_are_not_allowed_to/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqqtl3/ethnographic_poll_for_abangs_and_adiks_of_indian/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cr04a4/quah_jing_wen_smashes_national_200m_fly_record_by/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cr0h5j/set_minimum_age_for_women_seeking_abortion/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqxxwc/40_years_of_birthday_parties_at_swensens/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqxsl2/nus_new_measures_on_consent_education_are_still/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqntyb/big_data_helps_suss_assess_student_dropout_risk/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqn2rr/anyone_been_to_national_skin_centre/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cqmyb1/a_historical_brief_of_balestiers_red_chinese/?sort=confidence\n",
      "https://www.reddit.com/r/singapore/comments/cr2flx/lim_tean_circle_of_life_in_singapore/?sort=confidence\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "# Visit reddit.com/r/singapore home page using selenium\n",
    "# To obtain links to each post\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def getsoup(url):     \n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    data = driver.page_source\n",
    "    driver.close()\n",
    "  \n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "url = \"https://www.reddit.com/r/singapore/top\"\n",
    "# /top sorts the posts on the reddit singapore home page by most upvoted\n",
    "# if this program is repeated regularly, sort by New may be more appropriate\n",
    "\n",
    "soup = getsoup(url)\n",
    "\n",
    "# Get all hrefs from home page using bs4 \n",
    "\n",
    "posts = soup.find_all('a', {'data-click-id': 'body'})\n",
    "\n",
    "urls = []\n",
    "for post in posts:\n",
    "    current_url = post.get(\"href\")\n",
    "    urls.append( \"https://www.reddit.com\" + current_url + \"?sort=confidence\")\n",
    "\n",
    "print(*urls,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to every post's page using selenium\n",
    "# Store text from every post as a .csv file\n",
    "\n",
    "def cut_soup(list_a):\n",
    "    for string in list_a:\n",
    "        if \"More posts from the\" in string:\n",
    "            return (\" \".join(list_a[0:((list_a.index(string))+1)])) \n",
    "            break\n",
    "\n",
    "def appendall_to_csv(row):\n",
    "    f = open(\"post_text.csv\", \"a\", encoding='utf-8')\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow([row])\n",
    "    f.close()\n",
    "\n",
    "d = {}\n",
    "i = 0\n",
    "    \n",
    "for page in urls:    \n",
    "    driver = webdriver.Chrome()\n",
    "    page_soup = getsoup(page)\n",
    "    page_data = page_soup.prettify().splitlines()\n",
    "    d[\"page\" + str(i)] = page_data\n",
    "    i += 1\n",
    "    \n",
    "for key,value in d:\n",
    "    for r in value:\n",
    "        listno = [].append(r)\n",
    "    key = listno\n",
    "    # create new list \n",
    "    # append values to new list \n",
    "cleanlist = cut_soup(listno)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     cleanresultsall = cut_soup(listall)\n",
    "#     clean_resultsall = BeautifulSoup(cleanresultsall)\n",
    "    \n",
    "#     page_textall = page_soup.find_all(['p','h1'],text=True)\n",
    "#     page_textall = page_textall[1:]\n",
    "#     for result in page_textall:\n",
    "#         result_string = ''.join(result.findAll(text=True)) #convert result to string\n",
    "#         split_words = result_string.split()  # split string into words using split\n",
    "#         for word in split_words:\n",
    "#             appendall_to_csv(word.strip(string.punctuation)) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page0': None, 'page1': None, 'page2': None, 'page3': None, 'page4': None, 'page5': None, 'page6': None, 'page7': None, 'page8': None, 'page9': None, 'page10': None, 'page11': None, 'page12': None, 'page13': None, 'page14': None, 'page15': None, 'page16': None, 'page17': None, 'page18': None, 'page19': None, 'page20': None, 'page21': None, 'page22': None, 'page23': None, 'page24': None}\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"post_text.csv\") as f:\n",
    "    allwords = f.read().split()\n",
    "\n",
    "filtered_allwords = []\n",
    "\n",
    "for word in allwords:\n",
    "    if word not in MyStopWords:\n",
    "        filtered_allwords.append(word)\n",
    "        \n",
    "# write filtered_allwords to csv file\n",
    "\n",
    "with open(\"filtered_allwords.csv\", \"w+\") as g:\n",
    "    writer = csv.writer(g)\n",
    "    for word in filtered_allwords:\n",
    "        writer.writerow([word])\n",
    "\n",
    "        \n",
    "# Read the whole text.\n",
    "alltext = open(\"filtered_allwords.csv\").read()\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud().generate(alltext)\n",
    "\n",
    "# Display the generated image\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.figure(figsize=(200,200))\n",
    "wordcloud.to_file(\"allposts.png\")\n",
    "\n",
    "dct = {}\n",
    "\n",
    "for word in alltext.split():\n",
    "    dct[word] = alltext.split().count(word)\n",
    "\n",
    "alltable = pd.DataFrame(list(dct.items()),columns=['word','wordcount'])\n",
    "\n",
    "alltable1.sort_values(\"wordcount\",ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
